{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Complete LoRA & PEFT Implementation Guide - Notebook"
      ],
      "metadata": {
        "id": "cpyDH3ErnA9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is PEFT?\n",
        "* PEFT (Parameter-Efficient Fine-Tuning) is a family of techniques designed to fine-tune large language models efficiently by updating only a small subset of parameters rather than the entire model.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "* Traditional Fine-tuning: Updates all model parameters (billions of parameters)\n",
        "* PEFT: Updates only a small fraction (typically <1% of parameters)\n",
        "* Memory Efficient: Requires significantly less GPU memory\n",
        "* Storage Efficient: Only need to save the small adapter weights"
      ],
      "metadata": {
        "id": "5_dm-abJnGvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is LoRA?\n",
        "* LoRA (Low-Rank Adaptation) is a PEFT technique that decomposes weight updates into low-rank matrices, making fine-tuning extremely efficient.\n",
        "\n",
        "### Use Cases:\n",
        "\n",
        "* Domain Adaptation: Adapt general models to specific domains\n",
        "* Task-Specific Fine-tuning: Create specialized versions for different tasks\n",
        "* Personal Assistants: Create personalized model behavior\n",
        "* Resource-Constrained Environments: When full fine-tuning isn't feasible"
      ],
      "metadata": {
        "id": "XvAN0NxNnTAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Installation & Dependencies"
      ],
      "metadata": {
        "id": "EJ3nVq0Wnfm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate bitsandbytes peft\n",
        "!pip install pypdf2 sentence-transformers faiss-cpu\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Yu5Cv9r8mqDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Core Imports Library"
      ],
      "metadata": {
        "id": "g6l8df9cnhv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "import PyPDF2\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "JuOlkY5RHw8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: PDF Processing Pipeline"
      ],
      "metadata": {
        "id": "A0R-UUHJnobf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"Split text into overlapping chunks\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "hcHUn3kOHw5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Enhanced Vector Database"
      ],
      "metadata": {
        "id": "ayPTg-J7nraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVectorDB:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        self.encoder = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.chunks = []\n",
        "\n",
        "    def add_documents(self, texts):\n",
        "        \"\"\"Add text chunks to vector database\"\"\"\n",
        "        self.chunks = texts\n",
        "        embeddings = self.encoder.encode(texts)\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner product similarity\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def search(self, query, k=3):\n",
        "        \"\"\"Search for most relevant chunks\"\"\"\n",
        "        query_embedding = self.encoder.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        scores, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            results.append({\n",
        "                'text': self.chunks[idx],\n",
        "                'score': scores[0][i]\n",
        "            })\n",
        "        return results"
      ],
      "metadata": {
        "id": "0fOSmd62NFH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: LoRA Model Setup (Your Major Addition!)"
      ],
      "metadata": {
        "id": "oTmkcsJVnumn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_with_lora():\n",
        "    \"\"\"Setup model with LoRA configuration\"\"\"\n",
        "\n",
        "    # Quantization config for memory efficiency\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model_name = \"microsoft/DialoGPT-medium\"  # Lightweight model for Colab\n",
        "    # For better results, use: \"meta-llama/Llama-2-7b-chat-hf\" (requires HF token)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Add padding token if not present\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Prepare model for training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=16,  # Rank\n",
        "        lora_alpha=32,  # LoRA scaling parameter\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"c_attn\", \"c_proj\"],  # Target modules for LoRA\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to model\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "ddYaXQaKNJ6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Complete QA System Integration"
      ],
      "metadata": {
        "id": "DN2H4Gu5nyUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFQuestionAnswering:\n",
        "    def __init__(self):\n",
        "        self.vector_db = SimpleVectorDB()\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_pdf(self, pdf_path):\n",
        "        \"\"\"Load and process PDF\"\"\"\n",
        "        print(\"Extracting text from PDF...\")\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        print(\"Chunking text...\")\n",
        "        chunks = chunk_text(text)\n",
        "\n",
        "        print(\"Creating vector database...\")\n",
        "        self.vector_db.add_documents(chunks)\n",
        "\n",
        "        print(f\"Processed {len(chunks)} chunks from PDF\")\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize the model with LoRA\"\"\"\n",
        "        print(\"Setting up model with LoRA...\")\n",
        "        self.model, self.tokenizer = setup_model_with_lora()\n",
        "        print(\"Model setup complete!\")\n",
        "\n",
        "    def answer_question(self, question, max_length=200):\n",
        "        \"\"\"Answer question using retrieved context\"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = self.vector_db.search(question, k=3)\n",
        "\n",
        "        # Combine context\n",
        "        context = \"\\n\".join([chunk['text'] for chunk in relevant_chunks])\n",
        "\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"Based on the following context, answer the question:\n",
        "\n",
        "Context: {context[:1000]}...\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = response[len(prompt):].strip()\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'context': context[:500] + \"...\" if len(context) > 500 else context,\n",
        "            'relevant_chunks': len(relevant_chunks)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "UgzSRPttNNl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Initialize the QA system"
      ],
      "metadata": {
        "id": "5NCnQp0mn8Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize the QA system\n",
        "    qa_system = PDFQuestionAnswering()\n",
        "\n",
        "    # Upload your PDF file to Colab first\n",
        "    # You can use: files.upload() or mount Google Drive\n",
        "\n",
        "    # Load your e-commerce & cybersecurity PDF\n",
        "    pdf_path = \"Upload you File path\"\n",
        "\n",
        "    try:\n",
        "        qa_system.load_pdf(pdf_path)\n",
        "        qa_system.setup_model()\n",
        "\n",
        "        # Example questions for e-commerce & cybersecurity\n",
        "        questions = [\n",
        "            \"What payment security protocols are discussed?\"\n",
        "        ]\n",
        "\n",
        "        for question in questions:\n",
        "            print(f\"\\nQuestion: {question}\")\n",
        "            result = qa_system.answer_question(question)\n",
        "            print(f\"Answer: {result['context']}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Please upload your PDF file first!\")\n",
        "        print(\"Use: from google.colab import files; files.upload()\")"
      ],
      "metadata": {
        "id": "Mm8jwGm5NR_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Initialize question-answering session"
      ],
      "metadata": {
        "id": "0OWGiZcUoE53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_qa():\n",
        "    \"\"\"Interactive question-answering session\"\"\"\n",
        "    qa_system = PDFQuestionAnswering()\n",
        "\n",
        "    # Upload file\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your PDF file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the uploaded file path\n",
        "    pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Setup system\n",
        "    qa_system.load_pdf(pdf_path)\n",
        "    qa_system.setup_model()\n",
        "\n",
        "    print(\"\\nPDF loaded successfully! You can now ask questions.\")\n",
        "    print(\"Type 'quit' to exit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"Your question: \")\n",
        "        if question.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        result = qa_system.answer_question(question)\n",
        "        print(f\"\\nAnswer: {result['answer']}\\n\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "qffCJTW-NVA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Run and Test code"
      ],
      "metadata": {
        "id": "nIG2RxwsoNKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "J13-QQiRNXhc",
        "outputId": "3dfcefba-a435-403b-c933-884e6d46eae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF...\n",
            "Chunking text...\n",
            "Creating vector database...\n",
            "Processed 61 chunks from PDF\n",
            "Setting up model with LoRA...\n",
            "Model setup complete!\n",
            "\n",
            "Question: What payment security protocols are discussed?\n",
            "Answer: Security is a critical concern for electronic payment systems (EPS) such as credit cards, debit cards, e - wallets, eChecks and smart cards, as they involve the transfer of sensitive financial information over the internet. To protect against fraud and unauthorized transactions, EPS providers employ a variety of security measures, including: ï‚· Encryption : EPS providers use encryption to protect sensitive information as it is transmitted over the internet. This makes it difficult for hackers to ...\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsuZahNCNZ-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}